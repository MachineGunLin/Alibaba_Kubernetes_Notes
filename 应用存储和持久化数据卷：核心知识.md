# 应用存储和持久化数据卷：核心知识

# 一、Volumes介绍

### Pod Volumes

两个场景：

· 如果pod中的某一个容器在运行时异常退出，被kubelet重新拉起之后，如何保证之前容器产生的重要数据没有丢失？

· 如果同一个pod中的多个容器想要共享数据，应该如何做？

> 于是便有了Volumes。

Pod Volumes的常见类型：

1、本地存储：常用的有emptydir/hostpath；

2、网络存储：网络存储当前的实现方式有两种，一种是in-tree，它的实现的代码是放在K8S代码仓库中的，随着K8S对存储类型支持的增多，这种方式会给K8S本身的维护和发展带来很大的负担；第二种方式是out-of-tree，它的实现是与K8S自身解耦的，通过抽象接口将不同存储的driver实现从K8S代码仓库中剥离，因此out-of-tree是社区主推的一种实现网络存储插件的方式；

3、Projected Volumes：将一些配置信息，如secret/configmap用卷的形式挂载在容器中，让容器中的程序可以通过POSIX接口来访问配置数据；

4、PV和PVC。

![image-20200513102945262](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513102945262.png)

### Persistent Volumes

![image-20200513103014486](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513103014486.png)

PV就是Persistent Volumes。为什么需要有PV呢？我们知道pod中声明的volume生命周期和pod是相同的，考虑下面几个场景：

1、pod重建销毁，如用deployment管理的pod，在做镜像升级的过程中，会产生新的pod并且删除旧的pod，那新旧pod之间如何复用数据？

2、宿主机宕机的时候，要把上面的pod迁移，这时StatefulSet管理的pod，其实已经实现了带卷迁移的语义。这时通过pod volumes显然是做不到的；

3、多个pod之间想要共享数据应该如何去声明呢？我们知道同一个pod中的多个容器想要共享数据可以借助pod volumes去解决，但多个pod想要共享数据pod volumes就很难做到；

4、如果想要对数据卷做一些功能扩展性，比如snapshot、resize这些功能要如何做？

以上场景很难通过Pod volumes准确地表达它的复用/共享语义，扩展也比较困难。因此K8S中引入了Persistent Volumes的概念，它可以将存储和计算分离，通过不同的组件来管理存储资源和计算资源，然后解耦pod和volumes之间生命周期的关联。这样，当把pod删除之后，它使用的PV仍然存在，还可以被新建的pod复用。

### PVC设计意图

有了PV之后，要如何使用PV呢？于是便有了PVC。设计PVC的主要原因是为了简化K8S用户对存储的使用方式，做到职责分离。用户在使用存储的时候，只用声明所需的存储大小以及访问模式。

访问模式指的是：我要使用的存储是可以被多个node共享还是单node独占访问（注意是node level不是pod level）？只读还是读写访问？用户只用关心这些东西，无需关心与存储相关的实现细节。

通过PVC和PV的概念，将用户需求和实现细节解耦开，用户只通过PVC声明自己的存储需求。PV是由集群管理员和存储相关团队来统一运维和管控，这样就简化了用户使用存储的方式。可以看到，PV和PVC的设计其实有点像面向对象的借口与实现的关系。用户在使用功能时只需关心用户接口，无需关心它内部复杂的实现细节。

![image-20200513104418430](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513104418430.png)

既然PV是由集群管理员统一管控的，接下来看看PV这个对象怎么产生。

### Static Volume Provisioning

PV的第一种产生方式：静态产生方式——静态Provisioning。

静态Provisioning：由集群管理员事先去规划这个集群中的用户会怎样使用存储，它会先预分配一些存储，也就是预先创建一些PV；然后用户在提交自己的存储需求PVC的时候，K8S内部相关组件会帮助它把PVC和PV做绑定；之后用户再通过pod去使用存储的时候，就可以通过PVC找到相应的PV，它就可以使用了。

![image-20200513104926832](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513104926832.png)

静态产生方式的不足：首先需要集群管理员预分配，预分配其实很难预测用户的真实需求。比如用户需要的是20G，然后集群管理员在分配的时候可能有80G、100G的，但没有20G的，这样就很难满足用户的真实需求，也会造成资源浪费。

### Dynamic Volume Provisioning

第二种访问方式：动态产生方式Dynamic Volume Provisioning。

![image-20200513110619972](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513110619972.png)

动态供给就是，集群管理员不预分配PV，他写了一个模板文件，这个模板文件是用来表示创建某一类型存储（块存储、文件存储等）所需的一些参数，这些参数是用户不关心的、存储本身实现有关的参数。用户只需要提交自身的存储需求，也就是PVC文件，并在PVC中指定使用的存储模板（StorageClass）。

K8S集群中的管控组件，会结合PVC和StorageClass的信息动态生成用户所需要的存储（PV），将PVC和PV进行绑定后Pod就可以使用PV了。通过StorageClass配置生成存储所需要的存储模板，再结合用户的需求动态创建PV对象，做到按需分配，在没有增加用户使用难度的同时也解放了集群管理员的运维工作。



## 二、用例解读

看一下Pod Volumes、PV、PVC及StorageClass具体是如何使用的。

### Pod Volumes的使用

![image-20200513111547598](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513111547598.png)

如上图右侧所示，可以在pod yaml文件中的volumes字段中声明我们卷的名字以及卷的类型。声明的两个卷，一个是用的emptyDir另外一个用的是hostPath，这两种都是本地卷。在容器中如何使用这个卷呢？可以通过volumeMounts这个字段，volumeMounts字段里面指定的name其实就是它使用的哪个卷，mountPath就是容器中的挂载路径。

volumeMounts里还有个subPath字段。这两个容器都使用了同一个卷，就是名叫cache-volume的卷。在多个容器共享同一个卷的时候，为了隔离数据，可以通过subPath完成这个操作。它会在卷里面建立两个子目录，然后容器1往cache下面写的数据其实都写在子目录cache1(subPath: cache1)了，容器2往cache写的目录，其数据最终会落在这个卷里子目录下面的cache2(subPath: cache2)下。

还有一个readOnly字段，意思就是只读挂载，这个挂载点下面是没有办法去写数据的。

emptyDir和hostPath都是本地存储，有什么区别呢？

emptyDir其实是在pod创建的过程中会临时创建的一个目录，这个目录随着pod的删除也会被删除，里面的数据会被清空掉；hostPath顾名思义，其实就是宿主机上的一个路径，在pod删除之后，这个目录还是存在的，它的数据也不会被丢失。

### 静态PV使用

![image-20200513114139274](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513114139274.png)

静态PV首先是由管理员来创建的，这里以管理员NAS（阿里云文件存储）为例。首先需要现在阿里云的文件存储控制台上去创建NAS存储，然后把NAS存储的相关信息填到PV对象中，这个PV对象预创建出来后，用户可以通过PVC来声明自己的存储需求，然后再去创建pod。创建pod还是通过刚才讲解的字段把存储挂载到某一个容器中的某一个挂载点下面。

来看一下yaml文件。

![image-20200513115355797](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513115355797.png)

集群管理员首先是在云存储厂商那边先去把存储创建出来，然后把相应的信息填写到PV对象中。

创建的阿里云NAS文件存储对应的PV，有个比较重要的字段：capacity，即创建的这个存储的大小；accessMode是创建出来的这个存储的访问方式。reclaimpolicy就是这块存储在被使用后，等它的使用方pod以及PVC被删除之后，这个PV是应该被删除掉还是被保留呢，也就是PV的回收策略。

再看一下用户如何去使用PV对象。

![image-20200513120241682](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513120241682.png)

用户在使用存储的时候，需要先创建一个PVC对象。PVC对象里面，只需要指定存储需求，不用关心存储本身的具体实现细节。存储需求包括：需要的大小，也就是resources.requests.storage;还有访问方式，即需要这个存储的访问方式，例子中为ReadWriteMany，即支持多Node读写访问，这也是文件存储的典型特性。

上图左侧可以看到：声明中包含request.storage和accessModes，跟静态创建的PV是匹配的。这样的话在用户提交PVC的时候，K8S集群相关的组件就会把PV和PVC绑定到一起，之后在用户提交pod yaml的时候，可以在卷里面写上PVC的声明，在PVC声明里面可以通过claimName来声明要用哪个PVC。这时挂载方式和前面讲的一样，当提交完yaml的时候，它可以通过PVC找到绑定的那个PV，然后就可以用那块存储了。这是静态Provisioning到被pod使用的一个过程。

### 动态PV使用

上面提到了动态Provisioning，系统管理员不再预分配PV，而只是创建一个模板文件。这个模板文件叫StorageClass，在StorageClass里面需要填的重要信息：第一个是Provisioner，就是当创建PV和对应存储的时候，应该用哪个存储插件去创建。

![image-20200513121613084](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513121613084.png)

下面的那些参数是通过K8S创建存储的时候，需要指定的一些细节参数。对于这些参数，用户是不需要关心的，比如这里regionId、zoneId、fsType和type。ReclaimPolicy和刚才PV里的意思一样，就是动态创建出来的这块PV当使用方式用结束,pod及PVC被删除后，这块PV应该怎么处理，这里写的是delete，意思就是当使用方pod和PVC被删除之后，这个PV也会被删除掉。

再看一下集群管理员提交完模板文件StorageClass之后，用户怎么用？

首先还是需要写一个PVC的文件。

![image-20200513122134459](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513122134459.png)

PVC的文件里存储的大小、访问模式是不变的。现在需要新加一个字段，叫StorageClassName，它的意思是指定动态创建PV的模板文件的名字，这里StorageClassName填的就是上面声明的csi-disk。

提交完PVC之后，K8S集群中的相关组件就会根据PVC以及对应的StorageClass动态生成这块PV给这个PVC做一个绑定，之后用户在提交自己的yaml时，用法和接下来的流程和前面的静态使用方式是一样的，通过pvc找到我们动态创建的pv，然后把它挂载到相应的容器中就可以使用了。

### PV Spec重要字段解析

![image-20200513122548963](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513122548963.png)

· capacity: 存储对象的大小；

· AccessModes: 用户需要关心的、使用这个PV的方式。有三种使用方式：

​		· 第一种是单node读写访问；

​		· 第二种是多个node只读访问，是常见的一种数据的共享方式；

​		· 第三种是多个node上读写访问。

用户在提交PVC的时候最重要的两个字段就是capacity和AccessModes。在提交PVC后，K8S集群中的相关组件如何找到合适的pv呢？首先它是通过为PV建立的AccessModes索引找到所有能够满足用户的PVC里面的AccessModes要求的PV list，然后根据PVC的capacity, StorageClassName, label selector进一步筛选PV，如果满足条件的PV有多个，选择PV的size最小的，accessmodes列表最短的pv，也即最小适合原则。

· ReclaimPolicy：这个指的是用户PV的PVC在删除之后，PV应该如何处理？常见的有三种方式：

​		1、Recycle（回收，K8S已经不推荐用这种方式了）；

​		2、delete：PVC被删除之后PV也会被删除；

​		3、retain：保留。保留之后，后面这个PV需要管理员来手动处理。

· StorageClassName：动态provisioning时必须指定的一个字段，就是说到底用哪一个模板文件来生成PV；

· NodeAffinity：创建出来的PV能被哪些Node去挂载使用其实是有限制的。通过NodeAffinity来声明对node的限制，这样其实对使用该pv的pod调度也有限制，就是说pod必须要调度到这些能访问pv的node上，才能访问这块pv。

### PV状态流转

![image-20200513132302356](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513132302356.png)

创建PV对象后，它会处在短暂的pending状态；等真正的PV创建好后，它就处在available状态。

available状态意思就是可以使用的状态，用户在提交PVC之后，被K8S相关组件做bound（就是找到相应的PV），这个时候PV和PVC就结合到一起了，此时两者处在bound状态。当用户在使用完PVC将其删除后，这个PV就处在released状态，之后它应该被删除还是被保留就取决于ReclaimPolicy。

当PV已经处在released状态下时，是没有办法直接回到available状态的，也就是说接下来无法被一个新的PVC去做绑定。如果想要把已经released的PV复用，有两种方式：

1、可以新建一个PV对象，然后把之前released的PV的相关字段的信息填到新的PV对象里面，这样PV就可以结合新的PVC了；

2、删除pod之后不要去删除PVC对象，这样，给PV绑定的PVC还是存在的，下次pod 使用的时候就可以直接通过PVC去复用。K8S中的StatefulSet管理的pod带存储的迁移就是通过这种方式。



## 三、操作演示

因为没买阿里云盘的空间，这里直接贴官方文字版教程了。

### 静态 Provisioning 例子

 

静态 Provisioning 主要用的是阿里云的 NAS 文件存储；动态 Provisioning 主要用了阿里云的云盘。它们需要相应存储插件，插件我已经提前部署在我的 K8s 集群中了(csi-nasplugin*是为了在k8s中使用阿里云NAS所需的插件，csi-disk*是为了在k8s中使用阿里云云盘所需要的插件)。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142799-7b1ddb65-7430-4571-a9d9-3e40efad1eca.png)

 

我们接下来先看一下静态 Provisioning 的 PV 的 yaml 文件。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142922-6098bf94-06c1-4e9a-820e-3acb25e8717a.png)

 

volumeAttributes是我在阿里云nas控制台预先创建的 NAS 文件系统的相关信息，我们主要需要关心的有 capacity 为5Gi; accessModes 为多node读写访问; reclaimPolicy：Retain，也就是当我使用方的 PVC 被删除之后，我这个 PV 是要保留下来的；以及在使用这个卷的过程中使用的driver。

 

然后我们把对应的 PV 创建出来：

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142778-4413c063-dd80-4fa9-b106-b0f8248ed33a.png)

 

我们看一下上图 PV 的状态，已经处在 Available，也就是说它已经可以被使用了。

 

再创建出来 nas-pvc：

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142829-f428ba6b-bdb0-4f09-a897-7aef501c68ae.png)

 

我们看这个时候 PVC 已经新创建出来了，而且也已经和我们上面创建的PV绑定到一起了。我们看一下 PVC 的 yaml 里面写的什么。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142752-b2e587ba-f211-4476-b674-b46f42b8fc6b.png)

 

其实很简单 ，就是我需要的大小以及我需要的 accessModes。提交完之后，它就与我们集群中已经存在的 PV 做匹配，匹配成功之后，它就会做 bound。

 

接下来我们去创建使用 nas-fs 的 pod：

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142771-8e3b5746-9c5b-42f9-b305-b1f228f7ebe3.png)

 

上图看到，这两个 Pod 都已经处在 running 状态了。

 

我们先看一下这个 pod yaml：

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142775-36251fad-04f0-4308-9f9c-4cc06d60e16e.png)

 

pod yaml 里面声明了刚才我们创建出来的 PVC 对象，然后把它挂载到 nas-container 容器中的 /data 下面。我们这个 pod 是通过前面课程中讲解 deployment 创建两个副本，通过反亲和性，将两个副本调度在不同的 node 上面。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142793-ef81475d-3a92-450d-adb7-5ad1ac1f736a.png)

 

上图我们可以看一下，两个Pod所在的宿主机是不一样的。

 

如下图所示：我们登陆到第一个上面，findmnt 看一下它的挂载信息，这个其实就挂载在我声明的 nas-fs 上，那我们再在下面 touch 个 test.test.test 文件，我们也会登陆到另外一个容器看一下，它有没有被共享。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142785-3fa99dab-8a0d-4b24-9f1d-2923f4ec9f37.png)

 

我们退出再登陆另外一个 pod（刚才登陆的是第一个，现在登陆第二个）。

 

如下图所示：我们也 findmnt 一下，可以看到，这两个 pod 的远程挂载路径一样，也就是说我们用的是同一个 NAS PV，我们再看一下刚才创建出来的那个是否存在。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183143230-195ac117-7898-440c-a4cd-f0473c9b68c4.png)

 

可以看到，这个也是存在的，就说明这两个运行在不同node上的 pod 共享了同一个 nas 存储。

 

接下来我们看一下把两个 pod 删掉之后的情况。先删Pod，接着再删一下对应的 PVC (K8s 内部对 pvc 对象由保护机制，在删除 pvc 对象时如果发现有 pod 在使用 pvc，pvc 是删除不掉的)，这个可能要稍等一下。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142833-894c08f6-d5ec-47d2-b352-29d4d4bd8d7f.png)

 

看一下下图对应的 PVC 是不是已经被删掉了。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142924-79551ae3-35d0-41bd-bce9-6290664712b8.png)

 

上图显示，它已经被删掉了。再看一下，刚才的 nas PV 还是在的，它的状态是处在 Released 状态，也就是说刚才使用它的 PVC 已经被删掉了，然后它被 released 了。又因为我们 RECLAIN POLICY 是 Retain，所以它这个 PV 是被保留下来的。

 

### 动态 Provisioning 例子

 

接下来我们来看第二个例子，动态 Provisioning 的例子。我们先把保留下来的 PV 手动删掉，可以看到集群中没有 PV了。接下来演示一下动态 Provisioning。

 

首先，先去创建一个生成 PV 的模板文件，也就是 storageclass。看一下 storageclass 里面的内容，其实很简单。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142849-067f8f3f-362f-4a16-bd96-1b6ce4bf2f98.png)

 

如上图所示，我事先指定的是我要创建存储的卷插件(阿里云云盘插件，由阿里云团队开发)，这个我们已经提前部署好了；我们可以看到，parameters部分是创建存储所需要的一些参数，但是用户不需要关心这些信息；然后是 reclaimPolicy，也就是说通过这个 storageclass 创建出来的 PV 在给绑定到一起的 PVC 删除之后，它是要保留还是要删除。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183143027-a4adbf5f-8843-46c8-9ef7-de33c7ef8e41.png)

 

如上图所示：现在这个集群中是没有 PV 的，我们动态提交一个 PVC 文件，先看一下它的 PVC 文件。它的 accessModes-ReadWriteOnce (因为阿里云云盘其实只能是单 node 读写的，所以我们声明这样的方式），它的存储大小需求是 30G，它的 storageClassName 是 csi-disk，就是我们刚才创建的 storageclass，也就是说它指定要通过这个模板去生成 PV。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142941-c8871c9e-533a-494f-9cec-22e1a01e30e6.png)

 

这个 PVC 此时正处在 pending 状态，这就说明它对应的 PV 还在创建过程中。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142826-e13183e0-37b8-4d49-bf65-7adb2587134b.png)

 

稍过一会，我们看到已经有一个新的 PV 生成，这个 PV 其实就是根据我们提交的 PVC 以及 PVC 里面指定的storageclass 动态生成的。之后k8s会将生成的 PV 以及我们提交的 PVC，就是这个 disk PVC 做绑定，之后我们就可以通过创建 pod 来使用了。

 

再看一下 pod yaml：

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142973-db26a557-8349-4b2e-9129-38523f27b8f2.png)

 

pod yaml 很简单，也是通过 PVC 声明，表明使用这个 PVC。然后是挂载点，下面我们可以创建看一下。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142865-3b9f7f1d-b8ee-4f69-a945-dac9bdbc1dbb.png)

 

如下图所示：我们可以大概看一下 Events，首先被调度器调度，调度完之后，接下来会有个 attachdetach controller，它会去做 disk的attach操作，就是把我们对应的 PV 挂载到调度器调度的 node 上，然后Pod对应的容器才能启动，启动容器才能使用对应的盘。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142974-bde37cbb-0098-4d8c-a2f5-553daba2b63f.png)

 

接下来我会把 PVC 删掉，看一下PV 会不会根据我们的 reclaimPolicy 随之删掉呢？我们先看一下，这个时候 PVC 还是存在的，对应的 PV 也是存在的。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183142822-bfdeb6cd-ef50-4e3a-baa4-30e1a4ea392b.png)

 

然后删一下 PVC，删完之后再看一下：我们的 PV 也被删了，也就是说根据 reclaimPolicy，我们在删除 PVC 的同时，PV 也会被删除掉。

 

![img](https://intranetproxy.alipay.com/skylark/lark/0/2019/png/168324/1566183143032-f934fd52-657c-49c5-881a-570f1eb37af4.png)



## 四、架构设计

### PV和PVC的处理流程

K8S中PV和PVC体系的完整处理流程如下图。

![image-20200513134142570](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513134142570.png)

首先看一下右下部分的csi。csi全称为Container Storage Interface，它是K8S社区对存储插件实现(out-of-tree)的官方推荐方式。

csi的实现大体可以分为两部分：

· 第一部分是由K8S社区驱动实现的通用的部分，如图中的csi-provisioner和csi-attacher controller；

· 第二部分是由云存储厂商实现的，对接云存储厂商的OpenApi，主要是实现真正的create/delete/mount/unmount存储的相关操作，对应到上图中的csi-controller-server和csi-node-server。

看一下用户提交yaml之后K8S内部的处理流程。用户在提交PVC yaml的时候，首先会在集群中生成一个PVC对象，然后PVC对象会被csi-provisioner controller watch到，csi-provisioner会结合PVC对象以及PVC对象中声明的storageClass，通过GRPC调用csi-controller-server，然后到云存储服务这边去创建真正的存储，并最终创建出来PV对象。最后由集群中的PV controller将PVC和PV对象做bound之后，这个PV就可以被使用了。

用户在提交pod后，首先会被调度器调度选中一个合适的node，之后该node上面的kubelet在创建pod流程中会首先通过csi-node-server将我们之前创建的pv挂载到pod可以使用的路径，然后kubelet开始create并且start pod中所有的container。

### PV、PVC以及通过csi使用存储流程

![image-20200513135626515](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\image-20200513135626515.png)

主要分为三个阶段：

1、第一个阶段(create)是用户提交完PVC，由csi-provisioner创建存储，并生成PV对象，之后PV controller将PVC及生成的PV对象做bound，bound之后create阶段就完成了；

2、之后用户在提交pod的yaml的时候，首先会被调度选中某一个合适的node，等pod的运行node被选出来之后，会被AD Controller watch到pod选中的node，它会去查找pod中使用了哪些PV。然后它会生成一个内部的对象VolumeAttachment，从而去触发csi-attacher去调用csi-controller-server去做真正的attach操作，attach操作调到云存储厂商的OpenAPI。这个attach操作就是将存储attach到pod将会运行的node上面。第二个阶段attach阶段完成；

3、第三个阶段发生在kubelet创建pod的过程中，它在创建pod的过程中首先要去做一个mount，这里的mount操作是为了将已经attach到这个node上面的那块盘进一步mount到pod可以使用的一个具体路径，之后kubelet才开始创建并启动容器。这就是PV和PVC创建存储以及使用存储的第三个阶段-mount阶段。

小总结：第一个create阶段主要是创建存储；第二个attach阶段就是将那块存储挂载到node上面（通常将存储load到node的/dev下面）；第三个mount阶段将对应的存储进一步挂载到pod可以使用的路径。这就是PVC和PV通过CSI实现的卷从创建到使用的完整流程。